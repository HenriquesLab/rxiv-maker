name: Container APT Package Testing

on:
  # Trigger on APT-related changes
  pull_request:
    paths:
      - 'packaging/debian/**'
      - 'packaging/scripts/build-deb.sh'
      - 'packaging/scripts/setup-apt-repo.sh'
      - 'packaging/scripts/test-apt-container.sh'
      - 'packaging/scripts/run-container-tests.sh'
      - 'packaging/scripts/validate-apt-repo.sh'
      - '.github/workflows/publish-apt.yml'
      - '.github/workflows/test-apt-containers.yml'
      - 'packaging/tests/container/**'
      - 'packaging/tests/test_apt_container_workflow.py'

  # Trigger on release candidate validation
  push:
    branches:
      - 'release-*'
      - 'apt-repo'
    tags:
      - 'v*'

  # Weekly scheduled testing
  schedule:
    - cron: '0 2 * * 1'  # Every Monday at 2 AM UTC

  # Manual trigger
  workflow_dispatch:
    inputs:
      ubuntu_versions:
        description: 'Ubuntu versions to test (comma-separated)'
        required: false
        default: '20.04,22.04,24.04'
        type: string
      test_types:
        description: 'Test types to run (comma-separated)'
        required: false
        default: 'installation,functionality'
        type: choice
        options:
          - 'quick'
          - 'installation'
          - 'installation,functionality'
          - 'installation,functionality,security'
          - 'comprehensive'
      parallel_jobs:
        description: 'Number of parallel jobs'
        required: false
        default: '2'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
          - '4'
      fail_fast:
        description: 'Stop on first failure'
        required: false
        default: false
        type: boolean
      skip_repo_validation:
        description: 'Skip APT repository validation'
        required: false
        default: false
        type: boolean

env:
  # Container configuration
  CONTAINER_ENGINE: podman
  PODMAN_VERSION: '4.6'

  # Test configuration
  TEST_TIMEOUT: 1800  # 30 minutes per test
  DEBIAN_FRONTEND: noninteractive

  # Repository configuration
  REPO_BRANCH: apt-repo

jobs:
  # Pre-flight checks and validation
  validate-environment:
    name: Validate Test Environment
    runs-on: ubuntu-latest
    outputs:
      ubuntu-versions: ${{ steps.config.outputs.ubuntu-versions }}
      test-types: ${{ steps.config.outputs.test-types }}
      parallel-jobs: ${{ steps.config.outputs.parallel-jobs }}
      fail-fast: ${{ steps.config.outputs.fail-fast }}
      skip-repo-validation: ${{ steps.config.outputs.skip-repo-validation }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Configure test parameters
        id: config
        run: |
          # Set test configuration from inputs or defaults
          UBUNTU_VERSIONS="${{ github.event.inputs.ubuntu_versions || '20.04,22.04,24.04' }}"
          TEST_TYPES="${{ github.event.inputs.test_types || 'installation,functionality' }}"
          PARALLEL_JOBS="${{ github.event.inputs.parallel_jobs || '2' }}"
          FAIL_FAST="${{ github.event.inputs.fail_fast || 'false' }}"
          SKIP_REPO_VALIDATION="${{ github.event.inputs.skip_repo_validation || 'false' }}"

          # Adjust for pull requests (lighter testing)
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            UBUNTU_VERSIONS="22.04"
            TEST_TYPES="quick,installation"
            PARALLEL_JOBS="2"
          fi

          # Adjust for scheduled runs (comprehensive testing)
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            TEST_TYPES="comprehensive"
            PARALLEL_JOBS="3"
          fi

          echo "ubuntu-versions=$UBUNTU_VERSIONS" >> $GITHUB_OUTPUT
          echo "test-types=$TEST_TYPES" >> $GITHUB_OUTPUT
          echo "parallel-jobs=$PARALLEL_JOBS" >> $GITHUB_OUTPUT
          echo "fail-fast=$FAIL_FAST" >> $GITHUB_OUTPUT
          echo "skip-repo-validation=$SKIP_REPO_VALIDATION" >> $GITHUB_OUTPUT

          echo "Configuration:"
          echo "  Ubuntu versions: $UBUNTU_VERSIONS"
          echo "  Test types: $TEST_TYPES"
          echo "  Parallel jobs: $PARALLEL_JOBS"
          echo "  Fail fast: $FAIL_FAST"
          echo "  Skip repo validation: $SKIP_REPO_VALIDATION"

      - name: Validate test scripts
        run: |
          # Check that test scripts exist and are executable
          for script in packaging/scripts/test-apt-container.sh packaging/scripts/run-container-tests.sh packaging/scripts/validate-apt-repo.sh; do
            if [[ ! -f "$script" ]]; then
              echo "❌ Missing test script: $script"
              exit 1
            fi

            if [[ ! -x "$script" ]]; then
              echo "❌ Test script not executable: $script"
              exit 1
            fi

            echo "✅ Validated: $script"
          done

      - name: Validate container definitions
        run: |
          # Check container files
          for file in packaging/tests/container/Containerfile.ubuntu-apt-test packaging/tests/container/README.md; do
            if [[ ! -f "$file" ]]; then
              echo "❌ Missing container file: $file"
              exit 1
            fi
            echo "✅ Validated: $file"
          done

  # APT repository validation (optional)
  validate-apt-repository:
    name: Validate APT Repository
    runs-on: ubuntu-latest
    needs: validate-environment
    if: needs.validate-environment.outputs.skip-repo-validation != 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl gnupg bc

      - name: Validate APT repository
        run: |
          ./packaging/scripts/validate-apt-repo.sh \
            --repo-url "https://raw.githubusercontent.com/henriqueslab/rxiv-maker/apt-repo" \
            --output validation-results \
            --verbose

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: apt-repo-validation-results
          path: validation-results/
          retention-days: 7

  # Build local package for testing (if needed)
  build-test-package:
    name: Build Test Package
    runs-on: ubuntu-latest
    needs: validate-environment
    if: github.event_name == 'pull_request' || github.event_name == 'push'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0  # Need full history for version detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install build dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            debhelper \
            devscripts \
            dh-python \
            python3-dev \
            python3-setuptools \
            python3-wheel \
            lintian

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install hatch

      - name: Build Debian package
        run: |
          # Create output directory
          mkdir -p dist

          # Build package
          ./packaging/scripts/build-deb.sh --output dist --verbose

          # Validate package with lintian
          lintian dist/*.deb || true  # Don't fail on lintian warnings

      - name: Upload test package
        uses: actions/upload-artifact@v4
        with:
          name: test-package
          path: dist/*.deb
          retention-days: 7

  # Matrix container testing
  container-test-matrix:
    name: Container Test
    runs-on: ubuntu-latest
    needs: [validate-environment, validate-apt-repository]
    if: always() && (needs.validate-apt-repository.result == 'success' || needs.validate-apt-repository.result == 'skipped')

    strategy:
      matrix:
        ubuntu-version: ['22.04']
        test-type: ['quick']
      fail-fast: false
      max-parallel: 2

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Install Podman
        run: |
          sudo apt-get update
          sudo apt-get install -y podman

          # Configure Podman for rootless operation
          podman info

      - name: Download test package (if available)
        uses: actions/download-artifact@v4
        if: needs.build-test-package.result == 'success'
        with:
          name: test-package
          path: dist/

      - name: Run container test
        id: container-test
        run: |
          # Set test parameters
          TEST_CMD=(
            "./packaging/scripts/test-apt-container.sh"
            "--ubuntu-version" "${{ matrix.ubuntu-version }}"
            "--test-type" "${{ matrix.test-type }}"
            "--output" "test-results"
            "--timeout" "${{ env.TEST_TIMEOUT }}"
            "--verbose"
          )

          # Add local package if available
          if [[ -f dist/*.deb ]]; then
            LOCAL_PACKAGE=$(ls dist/*.deb | head -1)
            TEST_CMD+=("--local-package" "$LOCAL_PACKAGE")
            echo "Using local package: $LOCAL_PACKAGE"
          fi

          # Run test with timeout protection
          timeout $((TEST_TIMEOUT + 300)) "${TEST_CMD[@]}"

        env:
          CONTAINER_ENGINE: ${{ env.CONTAINER_ENGINE }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.ubuntu-version }}-${{ matrix.test-type }}
          path: test-results/
          retention-days: 14

      - name: Upload container logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: container-logs-${{ matrix.ubuntu-version }}-${{ matrix.test-type }}
          path: |
            test-results/**/*.log
            /tmp/podman-*
          retention-days: 7

  # Multi-container orchestrated testing
  multi-container-test:
    name: Multi-Container Orchestrated Test
    runs-on: ubuntu-latest
    needs: [validate-environment, validate-apt-repository]
    if: always() && (needs.validate-apt-repository.result == 'success' || needs.validate-apt-repository.result == 'skipped') && github.event_name != 'pull_request'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Install Podman
        run: |
          sudo apt-get update
          sudo apt-get install -y podman bc

          # Configure Podman
          podman info

      - name: Download test package (if available)
        uses: actions/download-artifact@v4
        if: needs.build-test-package.result == 'success'
        with:
          name: test-package
          path: dist/

      - name: Run multi-container test orchestration
        run: |
          # Set orchestration parameters
          ORCHESTRATION_CMD=(
            "./packaging/scripts/run-container-tests.sh"
            "--ubuntu-versions" "${{ needs.validate-environment.outputs.ubuntu-versions }}"
            "--test-types" "${{ needs.validate-environment.outputs.test-types }}"
            "--parallel" "${{ needs.validate-environment.outputs.parallel-jobs }}"
            "--output" "multi-container-results"
            "--format" "json"
            "--timeout" "${{ env.TEST_TIMEOUT }}"
            "--verbose"
          )

          # Add local package if available
          if [[ -f dist/*.deb ]]; then
            LOCAL_PACKAGE=$(ls dist/*.deb | head -1)
            ORCHESTRATION_CMD+=("--local-package" "$LOCAL_PACKAGE")
          fi

          # Add fail-fast if enabled
          if [[ "${{ needs.validate-environment.outputs.fail-fast }}" == "true" ]]; then
            ORCHESTRATION_CMD+=("--fail-fast")
          fi

          # Run orchestrated tests
          "${ORCHESTRATION_CMD[@]}"

        env:
          CONTAINER_ENGINE: ${{ env.CONTAINER_ENGINE }}

      - name: Upload multi-container results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: multi-container-test-results
          path: multi-container-results/
          retention-days: 14

  # Integration test suite
  integration-test-suite:
    name: Python Integration Test Suite
    runs-on: ubuntu-latest
    needs: [validate-environment, validate-apt-repository]
    if: always() && (needs.validate-apt-repository.result == 'success' || needs.validate-apt-repository.result == 'skipped')

    strategy:
      matrix:
        ubuntu-version: ['22.04']  # Run integration tests on primary version

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Podman
        run: |
          sudo apt-get update
          sudo apt-get install -y podman

      - name: Install Python test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-timeout

      - name: Download test package (if available)
        uses: actions/download-artifact@v4
        if: needs.build-test-package.result == 'success'
        with:
          name: test-package
          path: dist/

      - name: Run Python integration tests
        run: |
          # Set pytest arguments
          PYTEST_ARGS=(
            "packaging/tests/test_apt_container_workflow.py"
            "-v"
            "--tb=short"
            "--ubuntu-version=${{ matrix.ubuntu-version }}"
          )

          # Add coverage if running comprehensive tests
          if [[ "${{ needs.validate-environment.outputs.test-types }}" == *"comprehensive"* ]]; then
            PYTEST_ARGS+=("--cov=tests/integration")
          fi

          # Run tests
          python -m pytest "${PYTEST_ARGS[@]}"

        env:
          CONTAINER_ENGINE: ${{ env.CONTAINER_ENGINE }}
          PYTHONPATH: ${{ github.workspace }}/packaging/tests

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results-${{ matrix.ubuntu-version }}
          path: |
            .coverage
            htmlcov/
            pytest.xml
          retention-days: 14

  # Success gate for path-based skipping
  skip-success:
    name: Skip Success Gate
    runs-on: ubuntu-latest
    needs: validate-environment
    if: |
      needs.validate-environment.result == 'skipped' ||
      (github.event_name == 'push' && !contains(github.event.head_commit.modified, 'packaging/debian/') &&
       !contains(github.event.head_commit.modified, 'packaging/scripts/') &&
       !contains(github.event.head_commit.modified, '.github/workflows/') &&
       !contains(github.event.head_commit.modified, 'packaging/tests/container/'))

    steps:
      - name: Report successful skip
        run: |
          echo "## ⏭️ Container APT Testing Skipped Successfully" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Reason:** No APT-related changes detected" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ✅ Success (testing intelligently skipped)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Skip Conditions Met:" >> $GITHUB_STEP_SUMMARY
          echo "- ❌ No packaging/debian/ package changes" >> $GITHUB_STEP_SUMMARY
          echo "- ❌ No APT script changes" >> $GITHUB_STEP_SUMMARY
          echo "- ❌ No container test changes" >> $GITHUB_STEP_SUMMARY
          echo "- ❌ Not a scheduled or manual trigger" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "This is the expected behavior for efficiency - no container testing needed." >> $GITHUB_STEP_SUMMARY

  # Final reporting and status
  test-summary:
    name: Test Summary and Reporting
    runs-on: ubuntu-latest
    needs: [validate-environment, validate-apt-repository, container-test-matrix, multi-container-test, integration-test-suite, skip-success]
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        if: needs.validate-environment.result == 'success'

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        if: needs.validate-environment.result == 'success'
        with:
          path: all-test-results/

      - name: Generate comprehensive test report
        if: needs.validate-environment.result == 'success'
        run: |
          # Create summary report
          REPORT_FILE="test-summary-$(date +%Y%m%d-%H%M%S).md"

          cat > "$REPORT_FILE" << EOF
          # Container APT Package Testing Summary

          **Generated:** $(date -Iseconds)
          **Workflow:** ${{ github.workflow }}
          **Run ID:** ${{ github.run_id }}
          **Trigger:** ${{ github.event_name }}

          ## Configuration

          - **Ubuntu Versions:** ${{ needs.validate-environment.outputs.ubuntu-versions }}
          - **Test Types:** ${{ needs.validate-environment.outputs.test-types }}
          - **Parallel Jobs:** ${{ needs.validate-environment.outputs.parallel-jobs }}
          - **Container Engine:** ${{ env.CONTAINER_ENGINE }}

          ## Job Results

          | Job | Status | Notes |
          |-----|--------|-------|
          | Environment Validation | ${{ needs.validate-environment.result }} | Pre-flight checks |
          | APT Repository Validation | ${{ needs.validate-apt-repository.result }} | Repository accessibility and integrity |
          | Container Test Matrix | ${{ needs.container-test-matrix.result }} | Individual container tests |
          | Multi-Container Test | ${{ needs.multi-container-test.result }} | Orchestrated testing |
          | Integration Test Suite | ${{ needs.integration-test-suite.result }} | Python-based integration tests |

          ## Artifacts Generated

          EOF

          # List all artifacts
          if [[ -d "all-test-results" ]]; then
            echo "### Test Artifacts" >> "$REPORT_FILE"
            find all-test-results -name "*.json" -o -name "*.xml" -o -name "*.html" | \
              sed 's|all-test-results/||' | \
              sort | \
              sed 's/^/- /' >> "$REPORT_FILE"
          fi

          # Add status summary
          echo "" >> "$REPORT_FILE"
          echo "## Overall Status" >> "$REPORT_FILE"

          OVERALL_SUCCESS=true
          for job in validate-environment validate-apt-repository container-test-matrix multi-container-test integration-test-suite; do
            result_var="needs.${job//-/_}.result"
            if [[ "${{ needs.validate-environment.result }}" == "failure" ]] ||
               [[ "${{ needs.container-test-matrix.result }}" == "failure" ]] ||
               [[ "${{ needs.integration-test-suite.result }}" == "failure" ]]; then
              OVERALL_SUCCESS=false
              break
            fi
          done

          if [[ "$OVERALL_SUCCESS" == "true" ]]; then
            echo "✅ **All container tests passed successfully**" >> "$REPORT_FILE"
          else
            echo "❌ **Some container tests failed - review individual job results**" >> "$REPORT_FILE"
          fi

          echo "Generated report: $REPORT_FILE"
          cat "$REPORT_FILE"

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        if: needs.validate-environment.result == 'success'
        with:
          name: test-summary-report
          path: test-summary-*.md
          retention-days: 30

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request' && needs.validate-environment.result == 'success'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Find the summary report
            const summaryFiles = fs.readdirSync('.').filter(f => f.startsWith('test-summary-'));
            if (summaryFiles.length === 0) return;

            const summaryContent = fs.readFileSync(summaryFiles[0], 'utf8');

            // Create or update PR comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Container APT Package Testing Summary')
            );

            const commentBody = `## 🐳 Container APT Package Testing Summary

            ${summaryContent}

            <sub>Generated by workflow run [\`${{ github.run_id }}\`](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})</sub>`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }

      - name: Workflow success verification
        run: |
          # Ensure workflow reports success even when appropriately skipped
          if [ "${{ needs.skip-success.result }}" == "success" ]; then
            echo "✅ Container APT testing completed successfully (skipped appropriately)"
            exit 0
          elif [ "${{ needs.validate-environment.result }}" == "success" ]; then
            # Check if testing pipeline succeeded
            if [[ "${{ needs.validate-environment.result }}" == "failure" ]] ||
               [[ "${{ needs.container-test-matrix.result }}" == "failure" ]] ||
               [[ "${{ needs.integration-test-suite.result }}" == "failure" ]]; then
              echo "❌ Container APT package testing failed"
              exit 1
            else
              echo "✅ Container APT package testing completed successfully"
              exit 0
            fi
          else
            echo "✅ Container APT testing completed successfully"
            exit 0
          fi
