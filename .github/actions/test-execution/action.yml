name: 'Test Execution'
description: 'Reusable test runner with coverage and reporting'
inputs:
  test-type:
    description: 'Type of tests to run: unit, integration, performance, security'
    required: false
    default: 'unit'
  python-version:
    description: 'Python version for reporting'
    required: false
    default: '3.11'
  os:
    description: 'Operating system for reporting'
    required: false
    default: 'ubuntu-latest'
  coverage:
    description: 'Enable coverage reporting'
    required: false
    default: 'true'
  max-failures:
    description: 'Maximum number of test failures before stopping'
    required: false
    default: '20'
  timeout:
    description: 'Test timeout in seconds'
    required: false
    default: '300'
  upload-coverage:
    description: 'Upload coverage to Codecov'
    required: false
    default: 'false'

outputs:
  test-result:
    description: 'Test execution result (success/failure)'
    value: ${{ steps.run-tests.outputs.result }}
  coverage-file:
    description: 'Coverage file path'
    value: ${{ steps.run-tests.outputs.coverage-file }}

runs:
  using: 'composite'
  steps:
    - name: Windows-specific cleanup
      if: runner.os == 'Windows'
      shell: powershell
      run: |
        # Force cleanup of any stale processes and files
        Get-Process | Where-Object {$_.ProcessName -match "python|pytest|uv"} | Stop-Process -Force -ErrorAction SilentlyContinue

        # Clean temp directory
        $tempPath = [System.IO.Path]::GetTempPath()
        Get-ChildItem -Path $tempPath -Filter "*rxiv*" -Recurse -Force -ErrorAction SilentlyContinue | Remove-Item -Recurse -Force -ErrorAction SilentlyContinue
        Get-ChildItem -Path $tempPath -Filter "*pytest*" -Recurse -Force -ErrorAction SilentlyContinue | Remove-Item -Recurse -Force -ErrorAction SilentlyContinue

        # Set Windows file handling options
        [System.Environment]::SetEnvironmentVariable("PYTHONDONTWRITEBYTECODE", "1", "Process")
        [System.Environment]::SetEnvironmentVariable("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1", "Process")

        Write-Host "âœ… Windows cleanup completed"

    - name: Run unit tests
      if: inputs.test-type == 'unit'
      id: run-tests
      shell: bash
      run: |
        echo "Running unit tests with enhanced error reporting..."
        echo "Python version: $(python --version)"
        echo "UV version: $(uv --version)"
        echo "Working directory: $(pwd)"
        echo "Available memory: $(free -h 2>/dev/null || echo 'N/A')"

        # Set test result output
        echo "result=success" >> $GITHUB_OUTPUT
        echo "coverage-file=coverage.xml" >> $GITHUB_OUTPUT

        # Verify dependencies are installed
        uv pip list | head -10

        # Run tests with detailed output and continue on failure for better debugging
        set +e  # Don't exit on failure to capture exit code

        # Platform-specific pytest configuration with enhanced stability
        if [[ "${{ runner.os }}" == "Windows" ]]; then
          # Windows-specific flags to reduce file contention
          PYTEST_FLAGS="-n 1 --cache-clear --disable-warnings --tb=short --maxfail=5"
          export PYTEST_DISABLE_PLUGIN_AUTOLOAD=1
          export PYTHONDONTWRITEBYTECODE=1
        elif [[ "${{ runner.os }}" == "macOS" ]]; then
          # macOS-specific flags for stability (single worker to avoid fork issues)
          PYTEST_FLAGS="-n 1 --cache-clear --tb=short --disable-warnings --maxfail=5"
        else
          # Linux systems can use limited parallel execution
          PYTEST_FLAGS="-n 2 --cache-clear --tb=short --maxfail=10"
        fi

        # Add timeout protection
        PYTEST_FLAGS="$PYTEST_FLAGS --timeout=60 --timeout-method=thread"

        # Create a safer temporary directory for test artifacts
        export TEST_TEMP_DIR="${{ runner.temp }}/rxiv_tests"
        mkdir -p "$TEST_TEMP_DIR"

        # Exclude problematic performance tests, notebooks, and flaky tests for CI stability
        timeout 600 uv run pytest tests/ \
          -m "not performance and not flaky and not ci_exclude and not notebook" \
          --ignore=tests/performance/ \
          --ignore=tests/notebooks/ \
          --ignore=tests/integration/ \
          --cov=rxiv_maker \
          --cov-report=xml \
          --cov-report=term-missing \
          --cov-report=html:htmlcov \
          $PYTEST_FLAGS \
          --tb=line \
          -v \
          --durations=10 \
          --strict-markers \
          --junit-xml=pytest-results.xml \
          --basetemp="$TEST_TEMP_DIR" \
          || true  # Don't fail immediately to capture logs
        TEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error

        # Clean up test artifacts
        if [ -d "$TEST_TEMP_DIR" ]; then
          echo "Cleaning up test temporary directory..."
          rm -rf "$TEST_TEMP_DIR" || echo "Warning: Could not clean up temp directory"
        fi

        # Set result based on exit code (handle timeout specially)
        if [ $TEST_EXIT_CODE -eq 124 ]; then
          echo "result=timeout" >> $GITHUB_OUTPUT
          echo "::error::Tests timed out after 10 minutes"
        elif [ $TEST_EXIT_CODE -ne 0 ]; then
          echo "result=failure" >> $GITHUB_OUTPUT
        fi

        # Report test results even if some failed
        if [ -f "pytest-results.xml" ]; then
          echo "ðŸ“Š Test results summary:"
          TESTS=$(grep -o 'tests="[0-9]*"' pytest-results.xml | head -1 | grep -o '[0-9]*' || echo "0")
          FAILURES=$(grep -o 'failures="[0-9]*"' pytest-results.xml | head -1 | grep -o '[0-9]*' || echo "0")
          ERRORS=$(grep -o 'errors="[0-9]*"' pytest-results.xml | head -1 | grep -o '[0-9]*' || echo "0")
          SKIPPED=$(grep -o 'skipped="[0-9]*"' pytest-results.xml | head -1 | grep -o '[0-9]*' || echo "0")

          echo "  - Total tests: $TESTS"
          echo "  - Failures: $FAILURES"
          echo "  - Errors: $ERRORS"
          echo "  - Skipped: $SKIPPED"

          if [ "$FAILURES" != "0" ] || [ "$ERRORS" != "0" ]; then
            echo "::warning::$FAILURES failures and $ERRORS errors detected"
          fi
        else
          echo "::warning::No test results file found - tests may not have run"
        fi

        # Exit with original test exit code if tests failed
        exit ${TEST_EXIT_CODE:-0}
      env:
        PYTHONIOENCODING: utf-8
        PYTHONUTF8: 1
        PYTEST_TIMEOUT: ${{ inputs.timeout }}
        # Windows-specific environment for better file handling
        PYTEST_CURRENT_TEST: 1
        TMPDIR: ${{ runner.temp }}
        TMP: ${{ runner.temp }}
        TEMP: ${{ runner.temp }}

    - name: Run integration tests
      if: inputs.test-type == 'integration'
      shell: bash
      run: |
        echo "Running integration tests..."
        uv run pytest tests/integration/ -v --tb=short || echo "Integration tests not available"


    - name: Run security scans
      if: inputs.test-type == 'security'
      shell: bash
      run: |
        echo "ðŸ”’ Running security scans..."

        # Install security tools using uv
        uv add --dev bandit safety

        # Bandit scan
        if [ -d "src" ]; then
          bandit -r src/ -f json -o bandit-report.json || echo "Bandit completed with issues"
        fi

        # Safety scan
        safety scan --json --output safety-report.json || echo "Safety completed with issues"

        echo "âœ… Security scans completed"

    - name: Test CLI functionality
      if: inputs.test-type == 'unit'
      shell: bash
      run: |
        echo "Testing CLI installation and basic functionality..."
        uv run rxiv --version
        uv run rxiv --help
        uv run rxiv config show || echo "Config command not available"

    - name: Test package installation
      if: inputs.test-type == 'unit'
      shell: bash
      run: |
        # Build and test wheel installation
        uv build
        uv tool install dist/*.whl
        python -c "import rxiv_maker; print('âœ… Package imports successfully')"
        python -c "from rxiv_maker import __version__; print(f'Version: {__version__}')"

    - name: Upload coverage to Codecov
      if: inputs.upload-coverage == 'true' && inputs.python-version == '3.11' && inputs.os == 'ubuntu-latest'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
